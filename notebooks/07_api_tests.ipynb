{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import httpx \n",
    "from PIL import Image\n",
    "import io\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.11 environment at: /home/chris/repos/deepweeds-kerascv/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 47ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(n_batch: int):\n",
    "    images = []\n",
    "    for _ in range(n_batch):\n",
    "        img_bytes = io.BytesIO()\n",
    "        data = np.random.uniform(0, 255, (256,256,3)).astype(np.uint8)\n",
    "        Image.fromarray(data).save(img_bytes, \"JPEG\")\n",
    "        img_bytes.seek(0)\n",
    "        images.append(img_bytes)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get baselines running inference directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.5377483e-22, 1.1283496e-08, 8.8898247e-05, 1.1581302e-30,\n",
       "         4.4756601e-18, 2.5863977e-30, 1.1210388e-43, 5.5253008e-07,\n",
       "         9.9991047e-01]], dtype=float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_session = ort.InferenceSession('../models/MeNet.onnx', providers=[\"TensorrtExecutionProvider\"])\n",
    "onnx_input_name = onnx_session.get_inputs()[0].name\n",
    "onnx_output_name = onnx_session.get_outputs()[0].name\n",
    "preload_sample = np.random.uniform(0, 255, (1, 256, 256, 3)).astype(np.float32)\n",
    "onnx_session.run([onnx_output_name], {onnx_input_name: preload_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9 ms ± 54 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "preload_sample = np.random.uniform(0, 255, (1, 256, 256, 3)).astype(np.float32)\n",
    "_ = onnx_session.run([onnx_output_name], {onnx_input_name: preload_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc2fc1fa2c34decb3b64eb3faeee0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = [np.random.uniform(0, 255, (1, 256, 256, 3)).astype(np.float32) for _ in range(BATCH_SIZE)] \n",
    "for img in tqdm(images):\n",
    "    _ = onnx_session.run([onnx_output_name], {onnx_input_name: img})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Results running inference through our fastapi endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.71 ms ± 106 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "img_bytes = io.BytesIO()\n",
    "data = np.random.uniform(0, 255, (256,256,3)).astype(np.uint8)\n",
    "Image.fromarray(data).save(img_bytes, \"JPEG\")\n",
    "img_bytes.seek(0)\n",
    "\n",
    "url = \"http://localhost:8000/predict\"\n",
    "files = {\"file\": (\"image.jpg\", img_bytes, \"image/jpeg\")}\n",
    "\n",
    "response = requests.post(url, files=files)\n",
    "assert response.status_code == 200\n",
    "# print(response.status_code)\n",
    "# print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_image_request(image_bytes, client: httpx.AsyncClient):\n",
    "    # Convert numpy array to image bytes\n",
    "    files = {'file': ('image.jpg', image_bytes, 'image/jpeg')}\n",
    "    response = await client.post(\"/predict\", files=files)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1024 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:09<00:00, 109.24it/s]\n"
     ]
    }
   ],
   "source": [
    "images = gen_batch(BATCH_SIZE // 4)\n",
    "async with httpx.AsyncClient(base_url=\"http://localhost:8000/\", limits=httpx.Limits(max_connections=1024)) as client:\n",
    "    res = await tqdm_asyncio.gather(*[send_image_request(image, client) for image in images[:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTTPX is borked, I've no idea why it's so slow?? - doesn't make any requests with > 1024 coroutines even with a reasonable connection limit (so need to do some extra concurrency control probably), and is worse on both sync and async compared to standard requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = gen_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dee71fac57e4f4ca018333b1aafea14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for img_bytes in tqdm(images):\n",
    "    files = {\"file\": (\"image.jpg\", img_bytes, \"image/jpeg\")}\n",
    "    response = requests.post(\"http://localhost:8000/predict\", files=files)\n",
    "    assert response.status_code == 200, (response.status_code, response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "async def aio_request(img_bytes, session): \n",
    "    data = aiohttp.FormData()\n",
    "    data.add_field('file', \n",
    "                    img_bytes, \n",
    "                    filename='image.jpg',\n",
    "                    content_type='image/jpeg')\n",
    "    \n",
    "    async with session.post(\"http://localhost:8000/predict\", data=data) as response:\n",
    "        return await response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [00:11<00:00, 344.50it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "images = gen_batch(BATCH_SIZE)\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    res = await tqdm_asyncio.gather(*[aio_request(image, session) for image in images[:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our gpu usage is never above ~50% even through the high request rate, so data throughput / python overhead is a hard-limit - we can't feed the model fast enough.  Bumping the number of processes will ease this but annoyingly we need a separate model instance for each, maybe this would be not be a problem if we switched to a language that allows parallel threads\n",
    "\n",
    "Running the server with 4x uvicorn workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [00:04<00:00, 833.67it/s]\n"
     ]
    }
   ],
   "source": [
    "images = gen_batch(BATCH_SIZE)\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    res = await tqdm_asyncio.gather(*[aio_request(image, session) for image in images[:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Halves the runtime but 4x's the vram usage (way under the limit anyway though so we're fine, but for sure not ideal with larger models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to experiment with in random order:\n",
    "\n",
    "1. Try an optimised inference service that does clever things I don't have time to implement - (nvidia Triton, TFServing...)\n",
    "2. Try to figure out how to make the onnx inference session async compatible\n",
    "3. If no luck try to re-write inference service in cpp with threading & async (as opposed to async only) on the prediction endpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
