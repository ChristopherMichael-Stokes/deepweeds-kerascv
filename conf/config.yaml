print_summary: true
seed: 42
mixed_precision: mixed_bfloat16

data:
  val_split: 0.1
  dataset: "data/images/train"

model:
  seed: 42
  n_classes: 9
  input_shape: [256, 256, 3]
  output_activation: softmax
  block_dropout: 0.3
  blocks:
    - filters: 16
    - filters: 16
    - filters: 16
    - filters: 32
    - filters: 32
    - filters: 32
    - filters: 128
    - filters: 128
    - filters: 128

train:
  focal_loss: True
  loss_params:
    # label_smoothing: 0.1
    # alpha: [[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]]
    alpha: [[0.12705305, 0.01433119, 0.12518212, 0.1132095 , 0.12223666,
       0.13133574, 0.12484786, 0.11988596, 0.12191792]]
  augment: True 
  optimizer: AdamW
  optimizer_params:
    learning_rate: 1e-4
    weight_decay: 1e-1
  num_classes: 9
  metrics: [accuracy, precision, recall, auc]
  batch_size: 64
  early_stopping_params:
    monitor: loss
    patience: 10 
    restore_best_weights: True
  lr_schedule_params: 
    monitor: val_loss
    factor: 0.5
    patience: 15 
    cooldown: 20
  # Class weighting combined with focal alpha hurt performance, alpha on it's own gives better performance
  # class_weight:
  #   weight_divisor: 1.0
  #   weight_map:
  #     0: 1.9032306763285025
  #     1: 0.2146785866326096
  #     2: 1.8752045217908673
  #     3: 1.6958568738229756
  #     4: 1.8310820624546116
  #     5: 1.9673845193508115
  #     6: 1.870197300103842
  #     7: 1.7958689458689459
  #     8: 1.826307402578589
  train_args:
    epochs: 300

output:
  logdir: tf_logs

hydra:
  sweeper:
    params:
      train.lr_schedule_params.factor: 0.1,0.3,0.5,0.7,0.9
      train.optimizer_params.learning_rate: 1e-4,5e-5
